{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9d2aaa53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import nltk\n",
    "# from nltk import word_tokenize - нужно nltk.download('punkt')\n",
    "\n",
    "from nltk import wordpunct_tokenize, wordnet\n",
    "from nltk.stem import wordnet as WordNetLem\n",
    "from nltk.stem import SnowballStemmer, StemmerI\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import doc2vec\n",
    "from gensim.models import LdaModel, LdaMulticore\n",
    "from gensim.models import LsiModel\n",
    "from gensim import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6f27da38",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = \\\n",
    "    ['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', \n",
    "     'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', \n",
    "     'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', \n",
    "     'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', \n",
    "     'потом', 'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', \n",
    "     'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', \n",
    "     'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', \n",
    "     'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', \n",
    "     'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', \n",
    "     'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', \n",
    "     'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', \n",
    "     'им', 'более', 'всегда', 'конечно', 'всю', 'между']\n",
    "stop_words.extend(['очень', 'ооочень', 'это', 'данное'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b944417c",
   "metadata": {},
   "source": [
    "# Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9ff7669",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/coffee.csv')\n",
    "df['rating'] = df['rating'].astype('float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c834e7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = df.iloc[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298cfed3",
   "metadata": {},
   "source": [
    "# Токенизация, стемминг, удаление стоп слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8d273a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# лемматизация\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5a4ef806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['глубинка',\n",
       " 'страны',\n",
       " 'во',\n",
       " 'всех',\n",
       " 'своих',\n",
       " 'проявлениях',\n",
       " 'ассортимент',\n",
       " 'столовскии',\n",
       " 'интерьер',\n",
       " 'качество']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# есть проблема - nчто можно улучшить - маловато места посадки. n английская остается.\n",
    "tokenized = list(sent_to_words(df_tmp['text']))\n",
    "tokenized[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b48a3d72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['глубинк',\n",
       "  'стран',\n",
       "  'сво',\n",
       "  'проявлен',\n",
       "  'ассортимент',\n",
       "  'столовск',\n",
       "  'интерьер',\n",
       "  'качеств',\n",
       "  'цен',\n",
       "  'приемлем'],\n",
       " 47)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# стемминг и удаление стоп слов\n",
    "stemmer = SnowballStemmer('russian')\n",
    "\n",
    "stemming = []\n",
    "for sentence in tokenized:\n",
    "    stemming.append(list(stemmer.stem(word_x) for word_x in sentence if word_x not in stop_words))\n",
    "stemming[0][:10], len(stemming[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a50d9cf",
   "metadata": {},
   "source": [
    "# Кодирование (Tfidf / doc2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cda3053",
   "metadata": {},
   "source": [
    "## Sklearn Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f6d367a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'глубинк стран сво проявлен ассортимент столовск интерьер качеств цен приемлем средн бюджетно столово ссср чист нов хотел трасс поел желудок бастова знач риск оправда номер ночлег аналогичн толк пластиков окн нормальн закр штор окн тюл перв этаж плох мал заглядыва туалет душ совок повтор трасс руб соидет'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# объединим текст в отзывы\n",
    "tok_stem_text = []\n",
    "for word_x in stemming:\n",
    "    tok_stem_text.append(\" \".join(word_x))\n",
    "tok_stem_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "de93bef9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>цен приемлем</th>\n",
       "      <th>перв этаж</th>\n",
       "      <th>пил коф</th>\n",
       "      <th>пробова десерт</th>\n",
       "      <th>нчто улучш</th>\n",
       "      <th>удобн расположен</th>\n",
       "      <th>мест мал</th>\n",
       "      <th>вкусн коф</th>\n",
       "      <th>имен туд</th>\n",
       "      <th>вид мор</th>\n",
       "      <th>...</th>\n",
       "      <th>центр москв</th>\n",
       "      <th>атмосферны рестора</th>\n",
       "      <th>питер москв</th>\n",
       "      <th>the right</th>\n",
       "      <th>рок ролл</th>\n",
       "      <th>вкусн пив</th>\n",
       "      <th>нотдельн спасиб</th>\n",
       "      <th>мог сказа</th>\n",
       "      <th>кухн большо</th>\n",
       "      <th>похож муз</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.352424</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 686 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   цен приемлем  перв этаж  пил коф  пробова десерт  нчто улучш  \\\n",
       "0           0.0        0.0      0.0             0.0         0.0   \n",
       "1           0.0        0.0      0.0             0.0         0.0   \n",
       "2           0.0        0.0      0.0             0.0         0.0   \n",
       "3           0.0        0.0      0.0             0.0         0.0   \n",
       "4           0.0        0.0      0.0             0.0         0.0   \n",
       "\n",
       "   удобн расположен  мест мал  вкусн коф  имен туд  вид мор  ...  центр москв  \\\n",
       "0          0.000000       0.0        0.0       0.0      0.0  ...          0.0   \n",
       "1          0.000000       0.0        0.0       0.0      0.0  ...          0.0   \n",
       "2          0.000000       0.0        0.0       0.0      0.0  ...          0.0   \n",
       "3          0.000000       0.0        0.0       0.0      0.0  ...          0.0   \n",
       "4          0.352424       0.0        0.0       0.0      0.0  ...          0.0   \n",
       "\n",
       "   атмосферны рестора  питер москв  the right  рок ролл  вкусн пив  \\\n",
       "0                 0.0          0.0        0.0       0.0        0.0   \n",
       "1                 0.0          0.0        0.0       0.0        0.0   \n",
       "2                 0.0          0.0        0.0       0.0        0.0   \n",
       "3                 0.0          0.0        0.0       0.0        0.0   \n",
       "4                 0.0          0.0        0.0       0.0        0.0   \n",
       "\n",
       "   нотдельн спасиб  мог сказа  кухн большо  похож муз  \n",
       "0              0.0        0.0          0.0        0.0  \n",
       "1              0.0        0.0          0.0        0.0  \n",
       "2              0.0        0.0          0.0        0.0  \n",
       "3              0.0        0.0          0.0        0.0  \n",
       "4              0.0        0.0          0.0        0.0  \n",
       "\n",
       "[5 rows x 686 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coding_tfidf = TfidfVectorizer(min_df=2,          # минимальное количество вхождений слова\n",
    "                                ngram_range=(2,3),   # какие n-граммы учитывать\n",
    "                                #stop_words=stopwords.words(\"russian\")\n",
    "                                )\n",
    "\n",
    "res_vectorizer = coding_tfidf.fit_transform(tok_stem_text)\n",
    "\n",
    "# таблица частоты слов\n",
    "pd.DataFrame(res_vectorizer.toarray(), columns = coding_tfidf.vocabulary_.keys()).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c50476",
   "metadata": {},
   "source": [
    "## gensim Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5f4bd8f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(3751 unique tokens: ['аналогичн', 'ассортимент', 'бастова', 'бюджетно', 'глубинк']...)\n"
     ]
    }
   ],
   "source": [
    "#  сохранение извлеченных токенов в словарь\n",
    "my_dictionary = corpora.Dictionary(stemming)\n",
    "print(my_dictionary)\n",
    "\n",
    "# преобразование слов в Bag of Word\n",
    "bow_corpus =[my_dictionary.doc2bow(doc, allow_update = True) for doc in tokenized]\n",
    "# print(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1976fd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вес слова в корпусе Bag of Word перед применением TF-IDF (частота слов):\n",
    "word_weight =[]\n",
    "for doc in bow_corpus:\n",
    "    for id, freq in doc:\n",
    "        word_weight.append([my_dictionary[id], freq])\n",
    "# print(word_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "82caade6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вес слов после применения TF-IDF:\n",
    "# создать модель TF-IDF\n",
    "tfIdf = models.TfidfModel(bow_corpus, smartirs ='ntc')\n",
    "  \n",
    "# TF-IDF вес слова\n",
    "weight_tfidf =[]\n",
    "for doc in tfIdf[bow_corpus]:\n",
    "    for id, freq in doc:\n",
    "        weight_tfidf.append([my_dictionary[id], np.around(freq, decimals=3)])\n",
    "# print(weight_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "67a6f8da",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9584\\4286946044.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcorpus\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mlexicon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpora\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtfidf\u001b[0m   \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTfidfModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdictionary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlexicon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mvectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlexicon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9584\\4286946044.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcorpus\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mlexicon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpora\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtfidf\u001b[0m   \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTfidfModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdictionary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlexicon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mvectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlexicon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenize' is not defined"
     ]
    }
   ],
   "source": [
    "# corpus - токенизированный текст\n",
    "lexicon = gensim.corpora.Dictionary(corpus) \n",
    "tfidf   = gensim.models.TfidfModel(dictionary = lexicon, normalize = True) \n",
    "vectors = [tfidf[lexicon.doc2bow(doc)] for doc in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97803dd",
   "metadata": {},
   "source": [
    "## gensim doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "51c770a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для обучения модели нам нужен список целевых документов\n",
    "def tagged_document(list_of_ListOfWords):\n",
    "    for x, ListOfWords in enumerate(list_of_ListOfWords):\n",
    "        yield doc2vec.TaggedDocument(ListOfWords, [x])\n",
    "\n",
    "# Обновите модель\n",
    "\n",
    "# Инициализация модели\n",
    "d2v_model = doc2vec.Doc2Vec(vector_size=30, # длина вектора, которым будет представлено предложение\n",
    "                            min_count=2,    # min кол-во встречания слова в прпедложении для учета\n",
    "                            epochs=30,      # количество эпох\n",
    "                           )\n",
    "# новые данные\n",
    "data_new = list(tagged_document(stem_nltk))\n",
    "    \n",
    "# расширить словарный запас\n",
    "d2v_model.build_vocab(data_new)\n",
    "  \n",
    "# Обучение модели Doc2Vec\n",
    "d2v_model.train(data_new, total_examples=d2v_model.corpus_count, epochs=d2v_model.epochs)\n",
    "  \n",
    "# Анализ выходных данных\n",
    "# analyze = d2v_model.infer_vector(['Мама мыла раму'])\n",
    "# analyze\n",
    "\n",
    "doc2vec_vectorizer = np.array([d2v_model.infer_vector([text_x]) for text_x in tok_stem_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2b784952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00495361,  0.0052808 ,  0.01092705, ..., -0.00824159,\n",
       "        -0.00615475, -0.00055768],\n",
       "       [-0.0140129 , -0.00800666, -0.00662626, ..., -0.00389153,\n",
       "        -0.0033671 ,  0.00423033],\n",
       "       [-0.0021455 , -0.00324717,  0.00090764, ...,  0.01602945,\n",
       "         0.0151157 , -0.00076643],\n",
       "       ...,\n",
       "       [ 0.00360031, -0.00615906,  0.00997726, ..., -0.0113132 ,\n",
       "         0.00513004,  0.00636086],\n",
       "       [ 0.00088269,  0.01320916, -0.00771323, ..., -0.01584101,\n",
       "         0.0090939 , -0.00289908],\n",
       "       [-0.00350721,  0.01143662,  0.01065524, ...,  0.00695964,\n",
       "         0.01551486,  0.00590714]], dtype=float32)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2vec_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2640c6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "scal = MinMaxScaler()\n",
    "doc2vec_vectorizer = scal.fit_transform(doc2vec_vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139ab4e0",
   "metadata": {},
   "source": [
    "# Моделирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "18408def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 2), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 2), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1)]\n"
     ]
    }
   ],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(stem_nltk)\n",
    "\n",
    "# Create Corpus\n",
    "texts = stem_nltk\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b2011a6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.0049536107),\n",
       " (1, 0.0052808006),\n",
       " (2, 0.010927051),\n",
       " (3, -0.0015454899),\n",
       " (4, -0.0076337834),\n",
       " (5, 0.0069790105),\n",
       " (6, 0.012038878),\n",
       " (7, 0.0021290223),\n",
       " (8, -0.0003728688),\n",
       " (9, 0.009557775),\n",
       " (10, 0.013048987),\n",
       " (11, 0.0136705395),\n",
       " (12, 0.00030434132),\n",
       " (13, 0.0007641832),\n",
       " (14, 0.016414156),\n",
       " (15, 0.011137237),\n",
       " (16, -0.0064205476),\n",
       " (17, 0.005839382),\n",
       " (18, 0.008994835),\n",
       " (19, -0.01347044),\n",
       " (20, 0.0018960933),\n",
       " (21, -0.010848733),\n",
       " (22, -0.007888592),\n",
       " (23, 0.008643001),\n",
       " (24, -0.001959622),\n",
       " (25, -0.0028713148),\n",
       " (26, 0.0057159048),\n",
       " (27, -0.008241594),\n",
       " (28, -0.006154752),\n",
       " (29, -0.0005576819)]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(stem_nltk)\n",
    "corpus = [[(i, vec_x_x) for i, vec_x_x in zip(range(30), vec_x)] for vec_x in doc2vec_vectorizer]\n",
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f3cbd623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d1e318fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.000*\"заня\" + 0.000*\"замен\" + 0.000*\"крабов\" + 0.000*\"име\" + 0.000*\"им\" + '\n",
      "  '0.000*\"заяв\" + 0.000*\"лезут\" + 0.000*\"вилко\" + 0.000*\"выдернут\" + '\n",
      "  '0.000*\"мрачн\"'),\n",
      " (1,\n",
      "  '0.000*\"заня\" + 0.000*\"замен\" + 0.000*\"крабов\" + 0.000*\"име\" + 0.000*\"им\" + '\n",
      "  '0.000*\"заяв\" + 0.000*\"лезут\" + 0.000*\"вилко\" + 0.000*\"выдернут\" + '\n",
      "  '0.000*\"мрачн\"'),\n",
      " (2,\n",
      "  '0.000*\"заня\" + 0.000*\"замен\" + 0.000*\"крабов\" + 0.000*\"име\" + 0.000*\"им\" + '\n",
      "  '0.000*\"заяв\" + 0.000*\"лезут\" + 0.000*\"вилко\" + 0.000*\"выдернут\" + '\n",
      "  '0.000*\"мрачн\"'),\n",
      " (3,\n",
      "  '0.000*\"заня\" + 0.000*\"замен\" + 0.000*\"крабов\" + 0.000*\"име\" + 0.000*\"им\" + '\n",
      "  '0.000*\"заяв\" + 0.000*\"лезут\" + 0.000*\"вилко\" + 0.000*\"выдернут\" + '\n",
      "  '0.000*\"мрачн\"'),\n",
      " (4,\n",
      "  '0.000*\"заня\" + 0.000*\"замен\" + 0.000*\"крабов\" + 0.000*\"име\" + 0.000*\"им\" + '\n",
      "  '0.000*\"заяв\" + 0.000*\"лезут\" + 0.000*\"вилко\" + 0.000*\"выдернут\" + '\n",
      "  '0.000*\"мрачн\"'),\n",
      " (5,\n",
      "  '0.000*\"заня\" + 0.000*\"замен\" + 0.000*\"крабов\" + 0.000*\"име\" + 0.000*\"им\" + '\n",
      "  '0.000*\"заяв\" + 0.000*\"лезут\" + 0.000*\"вилко\" + 0.000*\"выдернут\" + '\n",
      "  '0.000*\"мрачн\"'),\n",
      " (6,\n",
      "  '0.000*\"заня\" + 0.000*\"замен\" + 0.000*\"крабов\" + 0.000*\"име\" + 0.000*\"им\" + '\n",
      "  '0.000*\"заяв\" + 0.000*\"лезут\" + 0.000*\"вилко\" + 0.000*\"выдернут\" + '\n",
      "  '0.000*\"мрачн\"'),\n",
      " (7,\n",
      "  '0.000*\"заня\" + 0.000*\"замен\" + 0.000*\"крабов\" + 0.000*\"име\" + 0.000*\"им\" + '\n",
      "  '0.000*\"заяв\" + 0.000*\"лезут\" + 0.000*\"вилко\" + 0.000*\"выдернут\" + '\n",
      "  '0.000*\"мрачн\"'),\n",
      " (8,\n",
      "  '0.000*\"заня\" + 0.000*\"замен\" + 0.000*\"крабов\" + 0.000*\"име\" + 0.000*\"им\" + '\n",
      "  '0.000*\"заяв\" + 0.000*\"лезут\" + 0.000*\"вилко\" + 0.000*\"выдернут\" + '\n",
      "  '0.000*\"мрачн\"'),\n",
      " (9,\n",
      "  '0.000*\"заня\" + 0.000*\"замен\" + 0.000*\"крабов\" + 0.000*\"име\" + 0.000*\"им\" + '\n",
      "  '0.000*\"заяв\" + 0.000*\"лезут\" + 0.000*\"вилко\" + 0.000*\"выдернут\" + '\n",
      "  '0.000*\"мрачн\"'),\n",
      " (10,\n",
      "  '0.000*\"заня\" + 0.000*\"замен\" + 0.000*\"крабов\" + 0.000*\"име\" + 0.000*\"им\" + '\n",
      "  '0.000*\"заяв\" + 0.000*\"лезут\" + 0.000*\"вилко\" + 0.000*\"выдернут\" + '\n",
      "  '0.000*\"мрачн\"'),\n",
      " (11,\n",
      "  '0.000*\"заня\" + 0.000*\"замен\" + 0.000*\"крабов\" + 0.000*\"име\" + 0.000*\"им\" + '\n",
      "  '0.000*\"заяв\" + 0.000*\"лезут\" + 0.000*\"вилко\" + 0.000*\"выдернут\" + '\n",
      "  '0.000*\"мрачн\"'),\n",
      " (12,\n",
      "  '0.000*\"заня\" + 0.000*\"замен\" + 0.000*\"крабов\" + 0.000*\"име\" + 0.000*\"им\" + '\n",
      "  '0.000*\"заяв\" + 0.000*\"лезут\" + 0.000*\"вилко\" + 0.000*\"выдернут\" + '\n",
      "  '0.000*\"мрачн\"'),\n",
      " (13,\n",
      "  '0.000*\"заня\" + 0.000*\"замен\" + 0.000*\"крабов\" + 0.000*\"име\" + 0.000*\"им\" + '\n",
      "  '0.000*\"заяв\" + 0.000*\"лезут\" + 0.000*\"вилко\" + 0.000*\"выдернут\" + '\n",
      "  '0.000*\"мрачн\"'),\n",
      " (14,\n",
      "  '0.000*\"заня\" + 0.000*\"замен\" + 0.000*\"крабов\" + 0.000*\"име\" + 0.000*\"им\" + '\n",
      "  '0.000*\"заяв\" + 0.000*\"лезут\" + 0.000*\"вилко\" + 0.000*\"выдернут\" + '\n",
      "  '0.000*\"мрачн\"'),\n",
      " (15,\n",
      "  '0.000*\"заня\" + 0.000*\"замен\" + 0.000*\"крабов\" + 0.000*\"име\" + 0.000*\"им\" + '\n",
      "  '0.000*\"заяв\" + 0.000*\"лезут\" + 0.000*\"вилко\" + 0.000*\"выдернут\" + '\n",
      "  '0.000*\"мрачн\"'),\n",
      " (16,\n",
      "  '0.000*\"заня\" + 0.000*\"замен\" + 0.000*\"крабов\" + 0.000*\"име\" + 0.000*\"им\" + '\n",
      "  '0.000*\"заяв\" + 0.000*\"лезут\" + 0.000*\"вилко\" + 0.000*\"выдернут\" + '\n",
      "  '0.000*\"мрачн\"'),\n",
      " (17,\n",
      "  '0.000*\"заня\" + 0.000*\"замен\" + 0.000*\"крабов\" + 0.000*\"име\" + 0.000*\"им\" + '\n",
      "  '0.000*\"заяв\" + 0.000*\"лезут\" + 0.000*\"вилко\" + 0.000*\"выдернут\" + '\n",
      "  '0.000*\"мрачн\"'),\n",
      " (18,\n",
      "  '0.000*\"заня\" + 0.000*\"замен\" + 0.000*\"крабов\" + 0.000*\"име\" + 0.000*\"им\" + '\n",
      "  '0.000*\"заяв\" + 0.000*\"лезут\" + 0.000*\"вилко\" + 0.000*\"выдернут\" + '\n",
      "  '0.000*\"мрачн\"'),\n",
      " (19,\n",
      "  '0.000*\"заня\" + 0.000*\"замен\" + 0.000*\"крабов\" + 0.000*\"име\" + 0.000*\"им\" + '\n",
      "  '0.000*\"заяв\" + 0.000*\"лезут\" + 0.000*\"вилко\" + 0.000*\"выдернут\" + '\n",
      "  '0.000*\"мрачн\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b12ffcba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -42.654178619384574\n",
      "\n",
      "Coherence Score:  0.8954992094499092\n"
     ]
    }
   ],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=stem_nltk, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
